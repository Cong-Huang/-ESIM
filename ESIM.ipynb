{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model \n",
    "from keras.optimizers import Nadam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft-attention\n",
    "\n",
    "def unchanged_shape(input_shape):\n",
    "    # function for Lambda layer \n",
    "    return input_shape\n",
    "\n",
    "def soft_attention_alignment(input_1, input_2):\n",
    "    \"\"\"\n",
    "    两输入为三维张量(bs, sl1, size) (bs, sl2, size)   (batch_size, seq_len, size)\n",
    "    \n",
    "    return (bs, sl1, size), (bs, sl2, size)\n",
    "    \"\"\"\n",
    "    attention = Dot(axes=-1)([input_1, input_2])  # (bs, sl1, size)·(bs, sl2, size) ==> (bs, sl1, sl2)\n",
    "    \n",
    "    w_att_1 = Lambda(lambda x: K.softmax(x, axis=1), output_shape=unchanged_shape)(attention)  # (bs, sl1, sl2)\n",
    "    w_att_2 = Permute((2, 1))(Lambda(lambda x: K.softmax(x, axis=2), \n",
    "                                     output_shape=unchanged_shape)(attention))  # (bs, sl2, sl1)\n",
    "    \n",
    "    in1_aligned = Dot(axes=1)([w_att_1, input_1])  # (bs, sl1, sl2)·(bs, sl1, size)  ==> (bs, sl2, size)\n",
    "    in2_aligned = Dot(axes=1)([w_att_2, input_2])  # (bs, sl2, sl1)·(bs, sl2, size)  ==> (bs, sl1, size)\n",
    "\n",
    "    return in1_aligned, in2_aligned   # (bs, sl2, size)  (bs, sl1, size)  与输入shape相反"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_corr(q1,q2,pool_way):\n",
    "    if pool_way == 'max':\n",
    "        pool = GlobalMaxPooling1D()\n",
    "    elif pool_way == 'ave':\n",
    "        pool = GlobalAveragePooling1D()\n",
    "    else:\n",
    "        raise RuntimeError(\"don't have this pool way\")\n",
    "\n",
    "    q1 = pool(q1)\n",
    "    q2 = pool(q2)\n",
    "\n",
    "    def norm_layer(x, axis=1):\n",
    "        return (x - K.mean(x, axis=axis, keepdims=True)) / K.std(x, axis=axis, keepdims=True)\n",
    "    q1 = Lambda(norm_layer)(q1)\n",
    "    q2 = Lambda(norm_layer)(q2)\n",
    "    \n",
    "    def jaccard(x):\n",
    "        return  x[0]*x[1]/(K.sum(x[0]**2,axis=1,keepdims=True)+\n",
    "                           K.sum(x[1]**2,axis=1,keepdims=True)-\n",
    "                           K.sum(K.abs(x[0]*x[1]),axis=1,keepdims=True))\n",
    "    merged = Lambda(jaccard)([q1,q2])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本匹配ESIM\n",
    "\n",
    "def esim():\n",
    "    text_len = 20\n",
    "    max_features = 20000\n",
    "    \n",
    "    q1 = Input(name='q1', shape=(text_len,))\n",
    "    q2 = Input(name='q2', shape=(text_len,))\n",
    "\n",
    "    embedding = Embedding(max_features, 100, input_length=text_len)\n",
    "\n",
    "    bn = BatchNormalization()\n",
    "    q1_embed = bn(embedding(q1))\n",
    "    q1_embed = SpatialDropout1D(0.2)(q1_embed)\n",
    "    q2_embed = bn(embedding(q2))\n",
    "    q2_embed = SpatialDropout1D(0.2)(q2_embed)\n",
    "\n",
    "    encode = Bidirectional(CuDNNLSTM(128, return_sequences=True), merge_mode='sum')\n",
    "    q1_encoded = encode(q1_embed)\n",
    "    q2_encoded = encode(q2_embed)\n",
    "\n",
    "    q1_aligned, q2_aligned = soft_attention_alignment(q1_encoded, q2_encoded)\n",
    "\n",
    "    q1_combined = Concatenate()([q1_encoded, q2_aligned, multiply([q1_encoded, q2_aligned])])\n",
    "    q2_combined = Concatenate()([q2_encoded, q1_aligned, multiply([q2_encoded, q1_aligned])])\n",
    "\n",
    "    compose = Bidirectional(CuDNNLSTM(128, return_sequences=True), merge_mode='sum')\n",
    "    q1_compare = compose(q1_combined)\n",
    "    q2_compare = compose(q2_combined)\n",
    "\n",
    "\n",
    "    merged_ave = pool_corr(q1_compare,q2_compare,'ave')\n",
    "    merged_max = pool_corr(q1_compare,q2_compare,'max')\n",
    "\n",
    "    merged = Concatenate()([merged_max, merged_ave])\n",
    "\n",
    "    dense = Dense(256, activation='relu')(merged)\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    out = Dense(1, activation='sigmoid')(dense)\n",
    "    lr = 0.0008\n",
    "\n",
    "    model = Model(inputs=[q1, q2], outputs=out)\n",
    "    model.compile(optimizer=Nadam(lr=lr), loss='binary_crossentropy', metrics=['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1 (InputLayer)                 (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2 (InputLayer)                 (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 100)      2000000     q1[0][0]                         \n",
      "                                                                 q2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 100)      400         embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 20, 100)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 20, 100)      0           batch_normalization_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 128)      235520      spatial_dropout1d_1[0][0]        \n",
      "                                                                 spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 20, 20)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 20, 128)      0           permute_1[0][0]                  \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 128)      0           lambda_1[0][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 20, 128)      0           bidirectional_1[1][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 384)      0           bidirectional_1[0][0]            \n",
      "                                                                 dot_3[0][0]                      \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20, 384)      0           bidirectional_1[1][0]            \n",
      "                                                                 dot_2[0][0]                      \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 20, 128)      526336      concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 128)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 128)          0           global_max_pooling1d_1[1][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 128)          0           global_average_pooling1d_1[1][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 128)          0           lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 128)          0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           lambda_8[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,844,561\n",
      "Trainable params: 2,844,361\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = esim()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
